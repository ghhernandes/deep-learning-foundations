{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "$$c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}$$\n",
    "\n",
    "To compute the matrix product of two tensors, we need three nested *for loops*. One for columns, one for rows and another for the sum of indices.\n",
    "\n",
    "We need to check that the number of columns of the first tensor is equals to numbers of rows of the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(10, 1)\n",
    "weights = torch.randn(1, 10)\n",
    "bias = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac == br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for k in range(ac):\n",
    "        for i in range(ar):\n",
    "            for j in range(bc):\n",
    "                c[i, j] += a[i, k] * b[k, j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0362,  0.4280,  0.2312,  0.0494, -0.7850,  0.1298, -0.1876, -0.0763,\n",
       "          0.0612,  0.2624],\n",
       "        [ 0.0912, -1.0788, -0.5828, -0.1245,  1.9788, -0.3271,  0.4728,  0.1924,\n",
       "         -0.1543, -0.6615],\n",
       "        [-0.0121,  0.1426,  0.0771,  0.0165, -0.2617,  0.0433, -0.0625, -0.0254,\n",
       "          0.0204,  0.0875],\n",
       "        [ 0.0376, -0.4444, -0.2401, -0.0513,  0.8153, -0.1348,  0.1948,  0.0793,\n",
       "         -0.0636, -0.2725],\n",
       "        [-0.1378,  1.6296,  0.8804,  0.1880, -2.9892,  0.4942, -0.7142, -0.2907,\n",
       "          0.2331,  0.9992],\n",
       "        [ 0.0843, -0.9966, -0.5384, -0.1150,  1.8281, -0.3022,  0.4368,  0.1778,\n",
       "         -0.1426, -0.6111],\n",
       "        [ 0.0059, -0.0696, -0.0376, -0.0080,  0.1277, -0.0211,  0.0305,  0.0124,\n",
       "         -0.0100, -0.0427],\n",
       "        [ 0.0574, -0.6788, -0.3667, -0.0783,  1.2451, -0.2058,  0.2975,  0.1211,\n",
       "         -0.0971, -0.4162],\n",
       "        [ 0.0159, -0.1884, -0.1018, -0.0217,  0.3456, -0.0571,  0.0826,  0.0336,\n",
       "         -0.0270, -0.1155],\n",
       "        [ 0.0040, -0.0471, -0.0254, -0.0054,  0.0864, -0.0143,  0.0206,  0.0084,\n",
       "         -0.0067, -0.0289]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul(inputs, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0362,  0.4280,  0.2312,  0.0494, -0.7850,  0.1298, -0.1876, -0.0763,\n",
       "          0.0612,  0.2624],\n",
       "        [ 0.0912, -1.0788, -0.5828, -0.1245,  1.9788, -0.3271,  0.4728,  0.1924,\n",
       "         -0.1543, -0.6615],\n",
       "        [-0.0121,  0.1426,  0.0771,  0.0165, -0.2617,  0.0433, -0.0625, -0.0254,\n",
       "          0.0204,  0.0875],\n",
       "        [ 0.0376, -0.4444, -0.2401, -0.0513,  0.8153, -0.1348,  0.1948,  0.0793,\n",
       "         -0.0636, -0.2725],\n",
       "        [-0.1378,  1.6296,  0.8804,  0.1880, -2.9892,  0.4942, -0.7142, -0.2907,\n",
       "          0.2331,  0.9992],\n",
       "        [ 0.0843, -0.9966, -0.5384, -0.1150,  1.8281, -0.3022,  0.4368,  0.1778,\n",
       "         -0.1426, -0.6111],\n",
       "        [ 0.0059, -0.0696, -0.0376, -0.0080,  0.1277, -0.0211,  0.0305,  0.0124,\n",
       "         -0.0100, -0.0427],\n",
       "        [ 0.0574, -0.6788, -0.3667, -0.0783,  1.2451, -0.2058,  0.2975,  0.1211,\n",
       "         -0.0971, -0.4162],\n",
       "        [ 0.0159, -0.1884, -0.1018, -0.0217,  0.3456, -0.0571,  0.0826,  0.0336,\n",
       "         -0.0270, -0.1155],\n",
       "        [ 0.0040, -0.0471, -0.0254, -0.0054,  0.0864, -0.0143,  0.0206,  0.0084,\n",
       "         -0.0067, -0.0289]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0362,  0.4280,  0.2312,  0.0494, -0.7850,  0.1298, -0.1876, -0.0763,\n",
       "          0.0612,  0.2624],\n",
       "        [ 0.0912, -1.0788, -0.5828, -0.1245,  1.9788, -0.3271,  0.4728,  0.1924,\n",
       "         -0.1543, -0.6615],\n",
       "        [-0.0121,  0.1426,  0.0771,  0.0165, -0.2617,  0.0433, -0.0625, -0.0254,\n",
       "          0.0204,  0.0875],\n",
       "        [ 0.0376, -0.4444, -0.2401, -0.0513,  0.8153, -0.1348,  0.1948,  0.0793,\n",
       "         -0.0636, -0.2725],\n",
       "        [-0.1378,  1.6296,  0.8804,  0.1880, -2.9892,  0.4942, -0.7142, -0.2907,\n",
       "          0.2331,  0.9992],\n",
       "        [ 0.0843, -0.9966, -0.5384, -0.1150,  1.8281, -0.3022,  0.4368,  0.1778,\n",
       "         -0.1426, -0.6111],\n",
       "        [ 0.0059, -0.0696, -0.0376, -0.0080,  0.1277, -0.0211,  0.0305,  0.0124,\n",
       "         -0.0100, -0.0427],\n",
       "        [ 0.0574, -0.6788, -0.3667, -0.0783,  1.2451, -0.2058,  0.2975,  0.1211,\n",
       "         -0.0971, -0.4162],\n",
       "        [ 0.0159, -0.1884, -0.1018, -0.0217,  0.3456, -0.0571,  0.0826,  0.0336,\n",
       "         -0.0270, -0.1155],\n",
       "        [ 0.0040, -0.0471, -0.0254, -0.0054,  0.0864, -0.0143,  0.0206,  0.0084,\n",
       "         -0.0067, -0.0289]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(inputs, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the time spend to calculate 10 neurons with 784 inputs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(10, 28*28)\n",
    "weights = torch.randn(10, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch matmul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 s ± 9.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 matmul(inputs, weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python matmul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 µs ± 13.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 inputs @ weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch matmul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 torch.matmul(inputs, weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations\n",
    "\n",
    "Operations (+, -, *, /, >, <, ==)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10)\n",
    "b = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False,  True,  True, False, False])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a < b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a < b).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60% of **a** are less than **b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius Norm (Matrix Normalization)\n",
    "\n",
    "The Frobenius norm, sometimes also called the Euclidean norm (a term unfortunately also used for the vector L^2-norm), is matrix norm of an m×n matrix A defined as the square root of the sum of the absolute squares of its elements. [Wolfram](https://mathworld.wolfram.com/FrobeniusNorm.html)\n",
    "\n",
    "$$\\|A\\|_\\text{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2} $$\n",
    "\n",
    "The Frobenius norm can also be considered as a vector norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 761 µs, sys: 1.2 ms, total: 1.97 ms\n",
      "Wall time: 3.65 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.8819)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frobeniusNorm(x):\n",
    "    a = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            a += x[i, j] * x[i, j] #sum\n",
    "    return a ** (1/2) #sqrt\n",
    "%time frobeniusNorm(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 614 µs, sys: 1.14 ms, total: 1.75 ms\n",
      "Wall time: 2.91 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.8819)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time (m*m).sum().sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.randn((2,4))\n",
    "m2 = torch.randn((4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmulv2(a, b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac == br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            c[i, j] += (a[i, :] * b[:, j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7821,  2.5351,  1.5442, -1.6701,  3.0583,  1.8754],\n",
       "        [-0.3904, -2.3032,  0.2099, -0.0456, -3.3459,  1.1390]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmulv2(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 µs ± 40.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 matmulv2(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.39ms vs 673ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules\n",
    "\n",
    "Is called broadcasting when Tensor arguments can be expanded to be of equal sizes, without making copies of the data.\n",
    "\n",
    "Rules:\n",
    "- each tensor has at least 1 dim\n",
    "- starting from trailing dimension, the dimensions must *be equals*, one of then 1, or one not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.empty(5,3,4,1) * torch.empty(  3,1,1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "broadcastable, all rules always hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-dfd909433a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "(torch.empty(5,2,4,1) * torch.empty(  3,1,1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not broadcastable, the 3th dimension not equals or one (2 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4]), torch.Size([4]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 5, 6, 7]), torch.Size([4]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([4,5,6,7])\n",
    "b, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to adds new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]),\n",
       " tensor([[1, 2, 3, 4]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None], a[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8, 10, 12, 14],\n",
       "        [12, 15, 18, 21],\n",
       "        [16, 20, 24, 28]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None] * b[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  6,  7,  8],\n",
       "        [ 6,  7,  8,  9],\n",
       "        [ 7,  8,  9, 10],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None] + b[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 4)\n",
    "b = torch.randn(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0152,  0.3527, -0.0039,  0.7114]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6194],\n",
       "        [-0.5350],\n",
       "        [-1.2621],\n",
       "        [ 1.0401]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:, None, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6287, -0.2184,  0.0024, -0.4406],\n",
       "        [-0.5431, -0.1887,  0.0021, -0.3806],\n",
       "        [-1.2812, -0.4451,  0.0050, -0.8978],\n",
       "        [ 1.0559,  0.3668, -0.0041,  0.7399]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a[0, None] * b[:, None, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matmul with broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmulv3(a, b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac == br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        c[i] = (a[i, :, None] * b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = matmulv3(m1, m2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5 µs ± 604 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit matmulv3(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, :, None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**(4, 1) and **b**(4, 2) dims, now is broadcastable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6287, -0.6657],\n",
       "        [-0.1887,  0.4390],\n",
       "        [ 0.0050, -0.0034],\n",
       "        [ 0.7399,  0.2949]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, :, None] * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein summation\n",
    "\n",
    "Einstein summation convention is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving notational brevity. [Wikipedia](https://en.wikipedia.org/wiki/Einstein_notation)\n",
    "\n",
    "There are essentially [three rules](https://mathworld.wolfram.com/EinsteinSummation.html):\n",
    "\n",
    "1. Repeated indices are implicitly summed over.\n",
    "\n",
    "2. Each index can appear at most twice in any term.\n",
    "\n",
    "3. Each term must contain identical non-repeated indices. \n",
    "\n",
    "Example:\n",
    "\n",
    "$$y = \\sum_{i = 1}^3 c_i x^i = c_1 x^1 + c_2 x^2 + c_3 x^3$$\n",
    "\n",
    "is simplified by the convention to:\n",
    "\n",
    "$$y = c_i x^i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c[i, j] += a[i, k] * b[k, j]\n",
    "def matmulv4(a, b): return torch.einsum('ik,kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "60.7 µs ± 77.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 matmulv4(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 107.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "48.9 µs ± 112 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 m1.matmul(m2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
