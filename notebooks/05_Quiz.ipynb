{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions (01-04 notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a Python Code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5776]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "w = torch.randn(1, 10)\n",
    "b = torch.randn(1)\n",
    "\n",
    "neuron = x @ w.t() + b\n",
    "neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.9725, 0.3088, 0.0000, 1.4561, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]]),\n",
       " tensor([[-0.1179, -0.4141,  0.9725,  0.3088, -0.0642,  1.4561, -0.8060, -0.9297,\n",
       "          -0.3240, -1.4377]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    return x.clamp_min(0.)\n",
    "\n",
    "ReLU(x), x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.6669, 0.0000, 1.2285, 0.0000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "w1 = torch.randn(5, 10)\n",
    "b1 = torch.randn(5)\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "layer1 = ReLU(linear(x, w1, b1))\n",
    "layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write the Python code for a dense layer in plain Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix multiplication:**\n",
    "\n",
    "$$c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}$$\n",
    "\n",
    "**ReLU**\n",
    "\n",
    "$$f(x) = x^+ = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.6669, 0.0000, 1.2285, 0.0000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    r = torch.zeros(x.shape[1])\n",
    "    for i, e in enumerate(x.squeeze()):\n",
    "        r[i] = e.item() if e.item() >= 0 else 0.\n",
    "    return r\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return matmul(x, w.t()) + b\n",
    "\n",
    "def matmul(a, b):\n",
    "    ar, ac = a.shape[0], a.shape[1]\n",
    "    br, bc = b.shape[0], b.shape[1]\n",
    "    c = torch.zeros(ar, bc)\n",
    "    assert ac==br  \n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(br):\n",
    "                c[i, j] += a[i, k] * b[k, j]\n",
    "    return c\n",
    "\n",
    "layer1 = ReLU(linear(x, w1, b1))\n",
    "layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is the hidden size of a layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What does the t method do in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t equals Transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 5)\n",
    "x, x.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Python is slow. Without optimization we stay depending of Python *nested Loops*, that's not fast.\n",
    "\n",
    "## 8. In matmul, why is ac==br?\n",
    "\n",
    "Numbers of *columns* of array A must be equals numbers of *rows* on array B. \n",
    "\n",
    "To do a matrix multiplication we need multiply each entire **A column** with entire **B row**, and to multiply 2 vectors they must have same size.\n",
    "\n",
    "## 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "Using the 'magic' command **%time**\n",
    "\n",
    "## 10. What is elementwise arithmetic?\n",
    "\n",
    "Is the application of some arithmetic operation in each element of an array with the another one.\n",
    "\n",
    "Works on tensors of any rank, as long as they have the same shape.\n",
    "\n",
    "## 11. Write the PyTorch code to test whether every element of *a* is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.8069,  0.4581,  0.6376, -0.2375,  0.3406]),\n",
       " tensor([-1.1001, -1.2028,  1.8080, -0.3195, -0.3702]),\n",
       " tensor([ True,  True, False,  True,  True]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5)\n",
    "b = torch.randn(5)\n",
    "a, b, a > b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. What is a rank-0 tensor? How do you convert it to plain Python data type?\n",
    "\n",
    "rank-0 tensor with a tensor without shape and dimension equals 0.\n",
    "\n",
    "For convert to plain Python data type we use the **.item()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1598), 0, torch.Size([]), -0.15981844067573547)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(())\n",
    "a, a.ndim, a.shape, a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. What does this return, and why?\n",
    "\n",
    "tensor([1, 2]) + tensor([1])\n",
    "\n",
    "Sum element 1 with each element of [1, 2], returning tensor([2, 3]).\n",
    "\n",
    "Using broadcasting, the Tensor([1]) are expanded to Tensor([1, 1]) and the arithmetic is applied (eq: Tensor([1, 2]) + Tensor([1, 1])).\n",
    "\n",
    "PyTorch doesn't create three copies of [1] in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 2]) + torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. What does this return, and why?\n",
    "\n",
    "tensor([1, 2]) + tensor([1, 2, 3])\n",
    "\n",
    "This didn't work. To broadcast, the number of elements of (b) must be equals 1 or equals the (a) tensor.\n",
    "\n",
    "## 15. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "With elementwise we can remove one of three nested loops summing all i-th and j-th elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.6669, 0.0000, 1.2285, 0.0000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul(a, b):\n",
    "    ar, ac = a.shape[0], a.shape[1]\n",
    "    br, bc = b.shape[0], b.shape[1]\n",
    "    c = torch.zeros(ar, bc)\n",
    "    assert ac==br  \n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "                c[i, j] = torch.sum(a[i, :] * b[:, j])\n",
    "    return c\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return matmul(x, w.t()) + b\n",
    "\n",
    "layer1 = ReLU(linear(x, w1, b1))\n",
    "layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. What are the broadcasting rules?\n",
    "\n",
    "Dimensions are compatible when:\n",
    "1. Have the same size, or\n",
    "2. one of the is 1\n",
    "\n",
    "A (3d array): 28 x 28 x 3\n",
    "B (1d array): 28 x 1  x 3\n",
    "Result:       28 x 28 x 3 (works)\n",
    "\n",
    "A (3d array): 28 x 28 x 3\n",
    "B (1d array): 28 x 3  x 3\n",
    "Result:       28 x 28 x 3 (don't work)\n",
    "\n",
    "## 17. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "## 18. How doees unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "## 19. How can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
